{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import json\n",
    "from statistics import mean\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, date\n",
    "import torch\n",
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n",
    "\n",
    "assert issubclass(PyroModule[nn.Linear], nn.Linear)\n",
    "assert issubclass(PyroModule[nn.Linear], PyroModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "companies = [('AMZN', 'Amazon'), ('AAPL', 'Apple'), ('MSFT', 'Microsoft'),\n",
    "             ('DIS', 'Disney'), ('GOOG', 'Google'), ('CVS', 'CVS'),\n",
    "             ('GE', 'General Electric'), ('SAN', 'Santander'),\n",
    "             ('GS', 'Goldman Sachs'), ('CICHY', 'China Construction Bank')]\n",
    "stock_data = []\n",
    "tweet_data = []\n",
    "for company in companies:\n",
    "    abbr = company[0]\n",
    "    stock_data.append(pd.read_csv('./COMS6998-Project/financial/' + abbr + '_financial.csv'))\n",
    "    curr = pd.read_csv('COMS6998-Project/sentiment/' + abbr + '_sentiment.csv')\n",
    "    times = []\n",
    "    for time in curr['Time']:\n",
    "        date = time.split()[0]\n",
    "        times.append(date)\n",
    "    curr['Time'] = times\n",
    "    tweet_data.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lag(stocks, tweets, lag):\n",
    "    prev_stock_close = []\n",
    "    curr_stock_close = []\n",
    "    avg_pos = []\n",
    "    avg_neg = []\n",
    "\n",
    "    prev_time = datetime.strptime(tweets['Time'][0], '%Y-%m-%d')\n",
    "    curr_time = None\n",
    "    for date in stocks['Date']:\n",
    "        if datetime.strptime(date, '%Y-%m-%d').date() < datetime.strptime(stocks['Date'][lag], '%Y-%m-%d').date():\n",
    "            continue\n",
    "        index = stocks[stocks['Date'] == date].index[0] # getting current date's index in series\n",
    "        prev_stock_close.append(stocks['Close'][index-lag])\n",
    "        curr_stock_close.append(stocks['Close'][index])\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for time in tweets['Time']:\n",
    "            start = datetime.strptime(copy.copy(time), '%Y-%m-%d')\n",
    "            curr_time = datetime.strptime(date, '%Y-%m-%d')\n",
    "            next_day = prev_time + timedelta(days = 1)\n",
    "            if lag == 1: # accounting for weekends, which 3 and 5-day lags skip over\n",
    "                if (start.date() >= prev_time.date()) and (start.date() < curr_time.date()):\n",
    "                    index = tweets[tweets['Time'] == time].index[0]\n",
    "                    pos.append(json.loads(tweets['Sentiment'][index])[0])\n",
    "                    neg.append(json.loads(tweets['Sentiment'][index])[1])\n",
    "            else:\n",
    "                if (start.date() >= prev_time.date()) and (start.date() < next_day.date()):\n",
    "                    index = tweets[tweets['Time'] == time].index[0]\n",
    "                    pos.append(json.loads(tweets['Sentiment'][index])[0])\n",
    "                    neg.append(json.loads(tweets['Sentiment'][index])[1])\n",
    "#             print(curr_time)\n",
    "        if len(pos) > 0:\n",
    "            avg_pos.append(mean(pos))\n",
    "        else:\n",
    "            avg_pos.append(0) # less popular companies may not have any tweets mentioning them on a particular day, so we add a 0-value sentiment\n",
    "        if len(neg) > 0:\n",
    "            avg_neg.append(mean(neg))\n",
    "        else:\n",
    "            avg_neg.append(0)\n",
    "        prev_time = copy.copy(curr_time)\n",
    "    \n",
    "    return prev_stock_close, curr_stock_close, avg_pos, avg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added data for Amazon at lag = 1 ...\n",
      "added data for Apple at lag = 1 ...\n",
      "added data for Microsoft at lag = 1 ...\n",
      "added data for Disney at lag = 1 ...\n",
      "added data for Google at lag = 1 ...\n",
      "added data for CVS at lag = 1 ...\n",
      "added data for General Electric at lag = 1 ...\n",
      "added data for Santander at lag = 1 ...\n",
      "added data for Goldman Sachs at lag = 1 ...\n",
      "added data for China Construction Bank at lag = 1 ...\n",
      "added data for Amazon at lag = 3 ...\n",
      "added data for Apple at lag = 3 ...\n",
      "added data for Microsoft at lag = 3 ...\n",
      "added data for Disney at lag = 3 ...\n",
      "added data for Google at lag = 3 ...\n",
      "added data for CVS at lag = 3 ...\n",
      "added data for General Electric at lag = 3 ...\n",
      "added data for Santander at lag = 3 ...\n",
      "added data for Goldman Sachs at lag = 3 ...\n",
      "added data for China Construction Bank at lag = 3 ...\n",
      "added data for Amazon at lag = 5 ...\n",
      "added data for Apple at lag = 5 ...\n",
      "added data for Microsoft at lag = 5 ...\n",
      "added data for Disney at lag = 5 ...\n",
      "added data for Google at lag = 5 ...\n",
      "added data for CVS at lag = 5 ...\n",
      "added data for General Electric at lag = 5 ...\n",
      "added data for Santander at lag = 5 ...\n",
      "added data for Goldman Sachs at lag = 5 ...\n",
      "added data for China Construction Bank at lag = 5 ...\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "lags = [1, 3, 5]\n",
    "lag1_data = []\n",
    "lag3_data = []\n",
    "lag5_data = []\n",
    "for l in lags:\n",
    "    for i in range(len(stock_data)):\n",
    "        prev_stock_close, curr_stock_close, avg_pos, avg_neg = calculate_lag(stock_data[i], tweet_data[i], l)\n",
    "        data = {'curr_close': curr_stock_close,\n",
    "                'prev_close': prev_stock_close,\n",
    "                'pos_sentiment': avg_pos,\n",
    "                'neg_sentiment': avg_neg}\n",
    "        df = pd.DataFrame(data, columns = ['curr_close','prev_close', 'pos_sentiment', 'neg_sentiment'])\n",
    "        print(\"added data for\", companies[i][1], \"at lag =\", l, \"...\")\n",
    "        if l == 1:\n",
    "            lag1_data.append(df)\n",
    "        elif l == 3:\n",
    "            lag3_data.append(df)\n",
    "        else:\n",
    "            lag5_data.append(df)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def var_model(x, y):\n",
    "def var_model(x, y, num_iterations):\n",
    "    \n",
    "    # Regression model\n",
    "    linear_reg_model = PyroModule[nn.Linear](2, 1)\n",
    "\n",
    "    # Define loss and optimize\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\n",
    "    # num_iterations = 1500\n",
    "\n",
    "    def train():\n",
    "        # run the model forward on the data\n",
    "        y_pred = linear_reg_model(x).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        return loss\n",
    "\n",
    "    for j in range(num_iterations):\n",
    "        loss = train()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in linear_reg_model.named_parameters():\n",
    "        print(name, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(df):\n",
    "    y = torch.tensor(df['curr_close'].values, dtype=torch.float)\n",
    "    pos_x = torch.tensor(df[['prev_close', 'pos_sentiment']].values, dtype=torch.float)\n",
    "    neg_x = torch.tensor(df[['prev_close', 'neg_sentiment']].values, dtype=torch.float)\n",
    "    return y, pos_x, neg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Amazon with positive sentiment: \n",
      "[iteration 0050] loss: 9060047.0000\n",
      "[iteration 0100] loss: 398935.1250\n",
      "Learned parameters:\n",
      "weight [[0.9682846 1.4361296]]\n",
      "bias [1.8256166]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.3122  , p=0.5768  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.3160  , p=0.5740  , df=1\n",
      "likelihood ratio test: chi2=0.3158  , p=0.5741  , df=1\n",
      "parameter F test:         F=0.3122  , p=0.5768  , df_denom=248, df_num=1\n",
      "\n",
      " Amazon with negative sentiment: \n",
      "[iteration 0050] loss: 166688.6562\n",
      "[iteration 0100] loss: 163808.7500\n",
      "Learned parameters:\n",
      "weight [[0.9945425  0.29776064]]\n",
      "bias [0.20102845]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.8893  , p=0.1705  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=1.9121  , p=0.1667  , df=1\n",
      "likelihood ratio test: chi2=1.9049  , p=0.1675  , df=1\n",
      "parameter F test:         F=1.8893  , p=0.1705  , df_denom=248, df_num=1\n",
      "\n",
      " Apple with positive sentiment: \n",
      "[iteration 0050] loss: 40235.1523\n",
      "[iteration 0100] loss: 272.3256\n",
      "Learned parameters:\n",
      "weight [[0.9505898  0.01200611]]\n",
      "bias [2.0296392]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.2250  , p=0.6357  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.2277  , p=0.6332  , df=1\n",
      "likelihood ratio test: chi2=0.2276  , p=0.6333  , df=1\n",
      "parameter F test:         F=0.2250  , p=0.6357  , df_denom=248, df_num=1\n",
      "\n",
      " Apple with negative sentiment: \n",
      "[iteration 0050] loss: 14946.5566\n",
      "[iteration 0100] loss: 172.3324\n",
      "Learned parameters:\n",
      "weight [[ 0.98750633 -0.00311293]]\n",
      "bias [0.82663727]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.5876  , p=0.2088  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=1.6068  , p=0.2049  , df=1\n",
      "likelihood ratio test: chi2=1.6017  , p=0.2057  , df=1\n",
      "parameter F test:         F=1.5876  , p=0.2088  , df_denom=248, df_num=1\n",
      "\n",
      " Microsoft with positive sentiment: \n",
      "[iteration 0050] loss: 136211.6875\n",
      "[iteration 0100] loss: 36635.4062\n",
      "Learned parameters:\n",
      "weight [[0.8432022  0.44891474]]\n",
      "bias [0.5396444]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.4447  , p=0.2305  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=1.4621  , p=0.2266  , df=1\n",
      "likelihood ratio test: chi2=1.4579  , p=0.2273  , df=1\n",
      "parameter F test:         F=1.4447  , p=0.2305  , df_denom=248, df_num=1\n",
      "\n",
      " Microsoft with negative sentiment: \n",
      "[iteration 0050] loss: 73001.5703\n",
      "[iteration 0100] loss: 5963.4263\n",
      "Learned parameters:\n",
      "weight [[0.93659467 0.1729225 ]]\n",
      "bias [1.002551]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=4.6745  , p=0.0316  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=4.7310  , p=0.0296  , df=1\n",
      "likelihood ratio test: chi2=4.6870  , p=0.0304  , df=1\n",
      "parameter F test:         F=4.6745  , p=0.0316  , df_denom=248, df_num=1\n",
      "\n",
      " Disney with positive sentiment: \n",
      "[iteration 0050] loss: 17393.2793\n",
      "[iteration 0100] loss: 2132.2559\n",
      "Learned parameters:\n",
      "weight [[0.98173887 0.10409296]]\n",
      "bias [0.34058022]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0255  , p=0.8733  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0258  , p=0.8724  , df=1\n",
      "likelihood ratio test: chi2=0.0258  , p=0.8724  , df=1\n",
      "parameter F test:         F=0.0255  , p=0.8733  , df_denom=248, df_num=1\n",
      "\n",
      " Disney with negative sentiment: \n",
      "[iteration 0050] loss: 103708.2969\n",
      "[iteration 0100] loss: 8550.0303\n",
      "Learned parameters:\n",
      "weight [[0.95035666 0.24923599]]\n",
      "bias [1.5629588]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.1105  , p=0.7398  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.1119  , p=0.7380  , df=1\n",
      "likelihood ratio test: chi2=0.1119  , p=0.7380  , df=1\n",
      "parameter F test:         F=0.1105  , p=0.7398  , df_denom=248, df_num=1\n",
      "\n",
      " Google with positive sentiment: \n",
      "[iteration 0050] loss: 3260997.7500\n",
      "[iteration 0100] loss: 441253.1562\n",
      "Learned parameters:\n",
      "weight [[0.95143497 1.5147401 ]]\n",
      "bias [1.9670653]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=7.1306  , p=0.0081  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=7.2169  , p=0.0072  , df=1\n",
      "likelihood ratio test: chi2=7.1151  , p=0.0076  , df=1\n",
      "parameter F test:         F=7.1306  , p=0.0081  , df_denom=248, df_num=1\n",
      "\n",
      " Google with negative sentiment: \n",
      "[iteration 0050] loss: 1151727.2500\n",
      "[iteration 0100] loss: 319896.0000\n",
      "Learned parameters:\n",
      "weight [[0.9727498 1.2906145]]\n",
      "bias [1.1259294]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=6.7596  , p=0.0099  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=6.8414  , p=0.0089  , df=1\n",
      "likelihood ratio test: chi2=6.7498  , p=0.0094  , df=1\n",
      "parameter F test:         F=6.7596  , p=0.0099  , df_denom=248, df_num=1\n",
      "\n",
      " CVS with positive sentiment: \n",
      "[iteration 0050] loss: 1477.5933\n",
      "[iteration 0100] loss: 254.2058\n",
      "Learned parameters:\n",
      "weight [[ 0.9970534  -0.00842102]]\n",
      "bias [0.39668348]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.3031  , p=0.2548  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=1.3188  , p=0.2508  , df=1\n",
      "likelihood ratio test: chi2=1.3154  , p=0.2514  , df=1\n",
      "parameter F test:         F=1.3031  , p=0.2548  , df_denom=248, df_num=1\n",
      "\n",
      " CVS with negative sentiment: \n",
      "[iteration 0050] loss: 16868.7461\n",
      "[iteration 0100] loss: 1243.9067\n",
      "Learned parameters:\n",
      "weight [[0.93781805 0.07403509]]\n",
      "bias [1.1739004]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.7967  , p=0.1813  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=1.8184  , p=0.1775  , df=1\n",
      "likelihood ratio test: chi2=1.8119  , p=0.1783  , df=1\n",
      "parameter F test:         F=1.7967  , p=0.1813  , df_denom=248, df_num=1\n",
      "\n",
      " General Electric with positive sentiment: \n",
      "[iteration 0050] loss: 360.7414\n",
      "[iteration 0100] loss: 17.8023\n",
      "Learned parameters:\n",
      "weight [[ 0.8564633  -0.00320087]]\n",
      "bias [1.5616019]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.8413  , p=0.3599  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.8514  , p=0.3561  , df=1\n",
      "likelihood ratio test: chi2=0.8500  , p=0.3566  , df=1\n",
      "parameter F test:         F=0.8413  , p=0.3599  , df_denom=248, df_num=1\n",
      "\n",
      " General Electric with negative sentiment: \n",
      "[iteration 0050] loss: 79.4964\n",
      "[iteration 0100] loss: 14.8490\n",
      "Learned parameters:\n",
      "weight [[ 9.0254813e-01 -8.8566361e-04]]\n",
      "bias [1.0144376]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0246  , p=0.8754  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0249  , p=0.8745  , df=1\n",
      "likelihood ratio test: chi2=0.0249  , p=0.8745  , df=1\n",
      "parameter F test:         F=0.0246  , p=0.8754  , df_denom=248, df_num=1\n",
      "\n",
      " Santander with positive sentiment: \n",
      "[iteration 0050] loss: 17.6351\n",
      "[iteration 0100] loss: 6.3720\n",
      "Learned parameters:\n",
      "weight [[0.61477166 0.00085804]]\n",
      "bias [1.673161]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.2838  , p=0.5947  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.2872  , p=0.5920  , df=1\n",
      "likelihood ratio test: chi2=0.2871  , p=0.5921  , df=1\n",
      "parameter F test:         F=0.2838  , p=0.5947  , df_denom=248, df_num=1\n",
      "\n",
      " Santander with negative sentiment: \n",
      "[iteration 0050] loss: 161.8683\n",
      "[iteration 0100] loss: 2.0749\n",
      "Learned parameters:\n",
      "weight [[0.9391065  0.00214235]]\n",
      "bias [0.26569998]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.6328  , p=0.4271  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.6405  , p=0.4235  , df=1\n",
      "likelihood ratio test: chi2=0.6397  , p=0.4238  , df=1\n",
      "parameter F test:         F=0.6328  , p=0.4271  , df_denom=248, df_num=1\n",
      "\n",
      " Goldman Sachs with positive sentiment: \n",
      "[iteration 0050] loss: 180232.8281\n",
      "[iteration 0100] loss: 33747.1367\n",
      "Learned parameters:\n",
      "weight [[0.9142328  0.40700513]]\n",
      "bias [0.8941322]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0914  , p=0.7627  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0925  , p=0.7611  , df=1\n",
      "likelihood ratio test: chi2=0.0924  , p=0.7611  , df=1\n",
      "parameter F test:         F=0.0914  , p=0.7627  , df_denom=248, df_num=1\n",
      "\n",
      " Goldman Sachs with negative sentiment: \n",
      "[iteration 0050] loss: 425556.1875\n",
      "[iteration 0100] loss: 136616.6250\n",
      "Learned parameters:\n",
      "weight [[0.8341494 0.9729746]]\n",
      "bias [1.0276145]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0799  , p=0.7777  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0808  , p=0.7762  , df=1\n",
      "likelihood ratio test: chi2=0.0808  , p=0.7762  , df=1\n",
      "parameter F test:         F=0.0799  , p=0.7777  , df_denom=248, df_num=1\n",
      "\n",
      " China Construction Bank with positive sentiment: \n",
      "[iteration 0050] loss: 132.8657\n",
      "[iteration 0100] loss: 11.2744\n",
      "Learned parameters:\n",
      "weight [[ 9.1658545e-01 -5.3115096e-04]]\n",
      "bias [1.3755873]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssr based F test:         F=0.0079  , p=0.9291  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0080  , p=0.9286  , df=1\n",
      "likelihood ratio test: chi2=0.0080  , p=0.9286  , df=1\n",
      "parameter F test:         F=0.0079  , p=0.9291  , df_denom=248, df_num=1\n",
      "\n",
      " China Construction Bank with negative sentiment: \n",
      "[iteration 0050] loss: 3276.1072\n",
      "[iteration 0100] loss: 22.9607\n",
      "Learned parameters:\n",
      "weight [[0.9106747  0.00511646]]\n",
      "bias [1.1714329]\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0034  , p=0.9533  , df_denom=248, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0035  , p=0.9530  , df=1\n",
      "likelihood ratio test: chi2=0.0035  , p=0.9530  , df=1\n",
      "parameter F test:         F=0.0034  , p=0.9533  , df_denom=248, df_num=1\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "for i in range(len(companies)):\n",
    "    y, pos_x, neg_x = convert_data(lag1_data[i])\n",
    "    print(\"\\n\", companies[i][1], \"with positive sentiment: \")\n",
    "    var_model(pos_x, y, iterations)\n",
    "    granger_test_result = grangercausalitytests(lag1_data[i][['prev_close','pos_sentiment']], maxlag=1, verbose=True)\n",
    "\n",
    "    print(\"\\n\", companies[i][1], \"with negative sentiment: \")\n",
    "    var_model(neg_x, y, iterations)\n",
    "    granger_test_result = grangercausalitytests(lag1_data[i][['prev_close','neg_sentiment']], maxlag=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# ax.plot(sample_stock['Date'], sample_stock['Close'], \"o\")\n",
    "# ax.set(xlabel='Date',\n",
    "#           ylabel='Closing Price',\n",
    "#           title='HK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro predictive model for evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
